{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceafd43b",
   "metadata": {},
   "source": [
    "## Training an RL Agent to Learn to Multiply\n",
    "\n",
    "This notebook presents a worked example of how to build an RL agent with bourbon. In this example we build an RL agent that is used in a wind turbine to learn how to position the rotors with respect to wind parameters to achieve an optimal output power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eda1b0",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First things first, let's import the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1bf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bourbon.steps import RLStep\n",
    "from bourbon.qtable import QTable\n",
    "from bourbon.q_learning import QL\n",
    "\n",
    "from random import randint\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d5e470",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m                     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is Alice annotation for 1\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      5\u001b[0m                     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is Bob annotation for 1\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSandra\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is Sandra annotation of 3\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     13\u001b[0m                 ]\n\u001b[1;32m     15\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "                    {'dataid': 1, 'username': 'Alice', 'annotation': 'This is Alice annotation for 1'},\n",
    "                    {'dataid': 1, 'username': 'Bob', 'annotation': 'This is Bob annotation for 1'},\n",
    "                    {'dataid': 1, 'username': 'Sandra', 'annotation': 'This is Sandra annotation of 1'},\n",
    "                    {'dataid': 2, 'username': 'Alice', 'annotation': 'This is Alice annotation for 2'},\n",
    "                    {'dataid': 2, 'username': 'Bob', 'annotation': 'This is Bob annotation for 2'},\n",
    "                    {'dataid': 2, 'username': 'Sandra', 'annotation': 'This is Sandra annotation of 2'},\n",
    "                    {'dataid': 3, 'username': 'Alice', 'annotation': 'This is Alice annotation for 3'},\n",
    "                    {'dataid': 3, 'username': 'Bob', 'annotation': 'This is Bob annotation for 3'},\n",
    "                    {'dataid': 3, 'username': 'Sandra', 'annotation': 'This is Sandra annotation of 3'},\n",
    "                ]\n",
    "\n",
    "dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd5ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speeds = np.random.uniform(10, 15, size=100)  # Simulated wind speeds (m/s)\n",
    "# For discrete action space\n",
    "wind_speed_categories = [\"Low Wind\", \"Moderate Wind\", \"High Wind\", \"Very High Wind\", \"Extreme Wind\"]\n",
    "power_curve = np.interp(wind_speeds, [3, 7, 12, 15], [0, 50, 300, 0])  # Power curve (kW)\n",
    "initial_pitch_angle = 0\n",
    "# For discrete action space\n",
    "pitch_angle_categories = [\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\", \"Category 5\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b048479",
   "metadata": {},
   "source": [
    "### Define Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981533b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotor angle is the angle of the rotor blades\n",
    "actions = [i for i in range(0, 17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946e045",
   "metadata": {},
   "source": [
    "### Define the State Space\n",
    "\n",
    "The first thing that we should consider is how do we want to map our problem to a state-action space. Since a multiplication table presents a nice 2-dimensional space, it makes a good example for the reinforcement learning state space. We define the state space with a list, and each index of the list is a number that expresses how many possible values that dimension has. For instance, to represent a one-digit multiplication table in the range of 1-9, the state space can be defined as: `[9, 9]`. Let's however, make this space smaller (5x5) for the sake of less computation and faster convergance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca402a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each state is represented by wind speed, pitch angle, previous action\n",
    "state_space = [len(wind_speed_categories), len(pitch_angle_categories), len(actions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087ecc",
   "metadata": {},
   "source": [
    "### Descretize wind speed and pitch angels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to discretize continuous wind speed into categories\n",
    "def discretize_wind_speed(wind_speed):\n",
    "    if wind_speed < 5:\n",
    "        return \"Low Wind\"\n",
    "    elif wind_speed < 10:\n",
    "        return \"Moderate Wind\"\n",
    "    elif wind_speed < 15:\n",
    "        return \"High Wind\"\n",
    "    elif wind_speed < 20:\n",
    "        return \"Very High Wind\"\n",
    "    else:\n",
    "        return \"Extreme Wind\"\n",
    "\n",
    "# Function to discretize continuous pitch angle into categories\n",
    "def discretize_pitch_angle(pitch_angle):\n",
    "    # Define your discretization logic here, e.g., based on bins or categories\n",
    "    if pitch_angle < 10:\n",
    "        return \"Category 1\"\n",
    "    elif pitch_angle < 20:\n",
    "        return \"Category 2\"\n",
    "    elif pitch_angle < 30:\n",
    "        return \"Category 3\"\n",
    "    elif pitch_angle < 40:\n",
    "        return \"Category 4\"\n",
    "    else:\n",
    "        return \"Category 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0964f",
   "metadata": {},
   "source": [
    "### Define the RL Step\n",
    "\n",
    "It's now time to define our RL step. An RL step should be a child of `bourbon.steps.RLStep` class and implement its two abstract methods, namely `get_state()` and `get_reward()`. In addition to the definition of states and actions, this is where you make the RL agent really specific to your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c151fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_state(wind_speed, pitch_angle, previous_action):\n",
    "    wind_speed_category = discretize_wind_speed(wind_speed)\n",
    "    pitch_angle_category = discretize_pitch_angle(pitch_angle)\n",
    "    return (wind_speed_category, pitch_angle_category, previous_action)\n",
    "\n",
    "\n",
    "\n",
    "def get_reward(state, action):\n",
    "    # Extract wind speed category, pitch angle category, and previous action from the state\n",
    "    wind_speed_category, pitch_angle_category, previous_action = state\n",
    "    \n",
    "    # Retrieve the actual wind speed and pitch angle values (if needed)\n",
    "    wind_speed_value = get_actual_wind_speed(wind_speed_category)  # Implement this function\n",
    "    pitch_angle_value = get_actual_pitch_angle(pitch_angle_category)  # Implement this function\n",
    "    \n",
    "    # Use the actual continuous values for more fine-grained calculations (if needed)\n",
    "    power_generated = calculate_power_output(wind_speed_value, pitch_angle_value)  # Implement this function\n",
    "\n",
    "    # Calculate the reward based on power generated and action\n",
    "    reward = power_generated - 10 * abs(action - pitch_angle_value)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyRlStep(RLStep):\n",
    "    def get_state(self):\n",
    "        a, b = gen_rand_nums()\n",
    "        state = [a, b]\n",
    "        return state\n",
    "    def get_reward(self, state, action):\n",
    "        prod = reduce((lambda x,y: x*y), state)\n",
    "        reward = 1/(abs(prod-action)+1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114dc050",
   "metadata": {},
   "source": [
    "In the above code, the state situation is simplified which matches this particular problem. To get the current state we simply choose a two random numbers in the range of 1-5 (ignoring the previous action and the enviroment) via `gen_rand_nums()` defined below. The reward is calculated by comparing the RL agent's prediction to the actual capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e16e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_nums():\n",
    "    num_1 = randint(1,5)\n",
    "    num_2 = randint(1,5)\n",
    "    return num_1, num_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926914ed",
   "metadata": {},
   "source": [
    "### Initiate and Train the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab86d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q_learning      - 150 - INFO - Training the RL agent...\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 119999/119999 [00:16<00:00, 7364.91it/s, Error (%)=4]\n"
     ]
    }
   ],
   "source": [
    "mrls = MyRlStep()\n",
    "qt = QTable(state_space=state_space, actions=actions)\n",
    "ql = QL(qtable=qt, rl_step=mrls)\n",
    "ql.train(num_epochs=120000,\n",
    "         get_correct_action=get_correct_action_for_multiply,\n",
    "        log_dir='rl_multiplication')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb36c08",
   "metadata": {},
   "source": [
    "Before being able to train our RL agent, we need to define one more function that is used in the training loop and its main purpose is to decide what is the best action to take in a given state. This function must be implemented with respect to the problem that you are solving. For instance, in this example, at each state the best action is simply the actual multiplication of the two indexes that represent the state. Hence, we can define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_action_for_multiply(state: list):\n",
    "    return state[0]*state[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba27b87",
   "metadata": {},
   "source": [
    "The error results are always stored in a tensorboard plot. Outside Notebooks, you can access the plots by simply running `tensorboard --logdir=rl_multiplication`. You can then access the tensorboard under http://localhost:6006/. To observe the plots in the Notebooks directly, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdbcd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d9de0cad82b86b2b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d9de0cad82b86b2b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir rl_multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd86e7",
   "metadata": {},
   "source": [
    "The RL agent seems to be trained and the error has dropped well and relatively fast which is expected for this simple problem. Let's now write a script that uses the trained RL agent and answers multiplication queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af610f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two numbers separated by a comma, to see their product: 2,3\n",
      "2 x 3 = 6\n",
      "Enter two numbers separated by a comma, to see their product: 3,3\n",
      "3 x 3 = 9\n",
      "Enter two numbers separated by a comma, to see their product: 3,4\n",
      "3 x 4 = 12\n",
      "Enter two numbers separated by a comma, to see their product: 5,5\n",
      "5 x 5 = 25\n",
      "Enter two numbers separated by a comma, to see their product: 4,5\n",
      "4 x 5 = 20\n",
      "Enter two numbers separated by a comma, to see their product: 2.2\n",
      "Input format seems to be wrong, please try again.\n",
      "Enter two numbers separated by a comma, to see their product: 2,2\n",
      "2 x 2 = 4\n",
      "Enter two numbers separated by a comma, to see their product: 1,2\n",
      "1 x 2 = 2\n",
      "Enter two numbers separated by a comma, to see their product: 1,1\n",
      "1 x 1 = 1\n",
      "Enter two numbers separated by a comma, to see their product: 1,3\n",
      "1 x 3 = 3\n",
      "Enter two numbers separated by a comma, to see their product: 3,5\n",
      "3 x 5 = 15\n",
      "Enter two numbers separated by a comma, to see their product: Stop\n",
      "Have a nice day! Bye.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input (\"Enter two numbers separated by a comma, to see their product: \")\n",
    "    if inp.lower() == \"stop\":\n",
    "        print(\"Have a nice day! Bye.\")\n",
    "        break\n",
    "    try:\n",
    "        a, b = inp.split(',')\n",
    "    except:\n",
    "        print('Input format seems to be wrong, please try again.')\n",
    "        continue\n",
    "    a = int(a)\n",
    "    b = int(b)\n",
    "    if (a > 5) or (b > 5):\n",
    "        print('The RL agent knows how to multiply only numbers <= 5.')\n",
    "        continue\n",
    "    state = [a,b]\n",
    "    best_action = ql.get_best_action(state)\n",
    "    print(f'{a} x {b} = {best_action}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
