{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea9c6d1",
   "metadata": {},
   "source": [
    "## Training an RL agent to predict country capitals\n",
    "\n",
    "This notebook presents a worked example of how to build an RL agent with Kandula. In this example we build an RL agent that learns what are the capitals of different countrys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2a666",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First things first, let's import the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kandula.steps import RLStep\n",
    "from kandula.qtable import QTable\n",
    "from kandula.q_learning import QL\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4932a7",
   "metadata": {},
   "source": [
    "For this example, in order to train the RL agent, we have downloaded a json file that contains the countris of the world and their capitals from [this repository](https://github.com/icyrockcom/country-capitals/blob/master/data/country-list-with-ids.json). For convenience, we downloaded the file into the [presources folder](https://github.com/meghdadFar/kandula/tree/main/resources) in this repository. Let's read this file and prune it so that it suits our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3c1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/country_capital.json\", \"r\") as fc:\n",
    "    capitals = json.load(fc)\n",
    "capitals_dict = {}\n",
    "country_index = {}\n",
    "index_country = {}\n",
    "i=1\n",
    "for jl in capitals:\n",
    "    capitals_dict[jl[\"country\"]] = jl[\"capital\"]\n",
    "    country_index[jl[\"country\"]] = i\n",
    "    index_country[i] = jl[\"country\"]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aea3af",
   "metadata": {},
   "source": [
    "In the above code, we first read the json lines into `capitals`, we then create three dictionaries from it: `capitals_dict` that maps the country names to their capitals, `country_index` that maps the country names to incremental indexes, and `index_country` that maps back indexes to country names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb55fca",
   "metadata": {},
   "source": [
    "### Define the State Space\n",
    "\n",
    "The first thing that we should consider is how do we want to map our problem to a state-action space. In this example, I consider that each contry represents a stae and hence, for 248 countries we will have a 1-dimensional state space of 248. If the space was 2-dimensional and say the first dimension had a size N and the second dimension has a size M, we could have defined the `state_space` variable as: `[N, M]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2c68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [248]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241f9ae",
   "metadata": {},
   "source": [
    "### Define the Actions\n",
    "\n",
    "The next thing is to define the actions. In this example, I consider guessing (the right) capital as an action that my RL agent is suppose to learn and hence, I define my actions to be a list of 248 capitals. If for instance, my RL agent was supposed to take one of the two actions of e.g. shifting grear up or shifting gear down, my actions variable would have been `[shift_gear_up, shift_gear_down]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4f7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [v for _, v in capitals_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebddd4",
   "metadata": {},
   "source": [
    "### Define the RL Step\n",
    "\n",
    "It's now time to define our RL step. An RL step should be a child of `kandula.steps.RLStep` class and implement its two abstract methods, namely `get_state()` and `get_reward()`. In addition to the definition of states and actions, this is where you make the RL agent really specific to your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d674e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitalsRLStep(RLStep):\n",
    "\n",
    "    def get_state(self):\n",
    "        country = gen_rand_country()\n",
    "        state = [country_index[country]]\n",
    "        return state\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        s = reduce((lambda x: x), state)\n",
    "        reward = 1 if capitals_dict[index_country[s]] == action else 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364be8ce",
   "metadata": {},
   "source": [
    "In the above code, the state situation is simplified which matches this particular problem. To get the current state we simply choose a random country (ignoring the previous action and the possible changes of the enviroment) via `gen_rand_country()`. The reward is calculated by comparing the RL agent's prediction to the actual capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fdbd1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_country():\n",
    "    country, _ = random.choice(list(capitals_dict.items()))\n",
    "    return country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445a549",
   "metadata": {},
   "source": [
    "### Initiate and Train the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19651fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q_learning      - 150 - INFO - Training the RL agent...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 2499999/2499999 [06:06<00:00, 6825.26it/s, Error (%)=11.7]\n"
     ]
    }
   ],
   "source": [
    "mrls = CapitalsRLStep()\n",
    "qt = QTable(state_space=state_space, actions=actions)\n",
    "ql = QL(qtable=qt, rl_step=mrls)\n",
    "ql.train(2500000, get_correct_action_for_capitals, log_dir='rl_capitals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e739d5a",
   "metadata": {},
   "source": [
    "Before being able to train our RL agent, we need to define one more function that is used in the training loop and its main purpose is to decide what is the best action to take in a given state. You should be able to define this function with respect to the problem that you are solving. For instance, in this example, since we defined the state to be simply a country, the best action is to return the right capital for that country. Hence, we can define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540ee5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_action_for_capitals(state: List):\n",
    "    return capitals_dict[index_country[state[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0590f",
   "metadata": {},
   "source": [
    "The error results are always stored in a tensorboard plot. Outside Notebooks, you can access the plots by simply running `tensorboard --logdir=rl_capitals`. You can then access the tensorboard under http://localhost:6006/. To observe the plots in the Notebooks directly, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f791ae32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d8fc4bedfeefeab7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d8fc4bedfeefeab7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir rl_capitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a8fed",
   "metadata": {},
   "source": [
    "The RL agent seems to be trained and the error has dropped well, though it took quite a bit of iterations. Let's now write a script that uses the trained RL agent and answers queries about country capitals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3b821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your quey: What is the capital of Germany?\n",
      "Capital of Germany is Berlin\n",
      "Enter your quey: What is the capital of Australia?\n",
      "Capital of Australia is Canberra\n",
      "Enter your quey: What is the capital of France?\n",
      "Capital of France is Paris\n",
      "Enter your quey: What is the capital of Turkey?\n",
      "Capital of Turkey is Ankara\n",
      "Enter your quey: What is the capital of Egypt?\n",
      "Capital of Egypt is Cairo\n",
      "Enter your quey: Stop\n",
      "Have a nice day! Bye.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    country = \"\"\n",
    "    query = input (\"Enter your quey: \")\n",
    "    if query.lower() == \"stop\":\n",
    "        print(\"Have a nice day! Bye.\")\n",
    "        break\n",
    "    try:\n",
    "        tokens = word_tokenize(query)\n",
    "        for t in tokens:\n",
    "            if t in capitals_dict:\n",
    "                country = t\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        continue\n",
    "    try:\n",
    "        state = [country_index[country]]\n",
    "        best_action = ql.get_best_action(state)\n",
    "        print(f'Capital of {country} is {best_action}')\n",
    "    except:\n",
    "        print('Make sure the country name is written correctly, and is capitalized.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
