{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea9c6d1",
   "metadata": {},
   "source": [
    "## Training an RL agent to predict country capitals\n",
    "\n",
    "This notebook presents a worked example of how to build an RL agent with Kandula. In this example we build an RL agent that learns what are the capitals of different countrys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2a666",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First things first, let's import the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794bfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kandula import logging\n",
    "from kandula.steps import RLStep\n",
    "from kandula.qtable import QTable\n",
    "from kandula.Q_learning import QL\n",
    "from typing import List\n",
    "\n",
    "from functools import reduce\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4932a7",
   "metadata": {},
   "source": [
    "For this example, in order to train the RL agent, we have downloaded a json file that contains the countris of the world and their capitals from [this repository](https://github.com/icyrockcom/country-capitals/blob/master/data/country-list-with-ids.json). For convenience, we downloaded the file into the [presources folder](https://github.com/meghdadFar/kandula/tree/main/resources) in this repository. Let's read this file and prune it so that it suits our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3c1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/country_capital.json\", \"r\") as fc:\n",
    "    capitals: List = json.load(fc)\n",
    "capitals_dict = {}\n",
    "country_index = {}\n",
    "index_country = {}\n",
    "i=1\n",
    "for jl in capitals:\n",
    "    capitals_dict[jl[\"country\"]] = jl[\"capital\"]\n",
    "    country_index[jl[\"country\"]] = i\n",
    "    index_country[i] = jl[\"country\"]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aea3af",
   "metadata": {},
   "source": [
    "In the above code, we first read the json lines into `capitals`, we then create three dictionaries from it: `capitals_dict` that maps the country names to their capitals, `country_index` that maps the country names to incremental indexes, and `index_country` that maps back indexes to country names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb55fca",
   "metadata": {},
   "source": [
    "### Define the State Space\n",
    "\n",
    "The first thing that we should consider is how do we want to map our problem to a state-action space. In this example, I consider that each contry represents a stae and hence, for 248 countries we will have a 1-dimensional state space of 248. If the space was 2-dimensional and say the first dimension had a size N and the second dimension has a size M, we could have defined the `state_space` variable as: `[N, M]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2c68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [248]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241f9ae",
   "metadata": {},
   "source": [
    "### Define the Actions\n",
    "\n",
    "The next thing is to define the actions. In this example, I consider guessing (the right) capital as an action that my RL agent is suppose to learn and hence, I define my actions to be a list of 248 capitals. If for instance, my RL agent was supposed to take one of the two actions of e.g. shifting grear up or shifting gear down, my actions variable would have been `[shift_gear_up, shift_gear_down]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [v for _, v in capitals_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebddd4",
   "metadata": {},
   "source": [
    "### Define the RL Step\n",
    "\n",
    "It's now time to define our RL step. An RL step should be a child of `kandula.steps.RLStep` class and implement its two abstract methods, namely `get_state()` and `get_reward()`. In addition to the definition of states and actions, this is where you make the RL agent really specific to your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d674e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapitalsRLStep(RLStep):\n",
    "\n",
    "    def get_state(self):\n",
    "        country = gen_rand_country()\n",
    "        state = [country_index[country]]\n",
    "        return state\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        s = reduce((lambda x: x), state)\n",
    "        reward = 1 if capitals_dict[index_country[s]] == action else 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364be8ce",
   "metadata": {},
   "source": [
    "In the above code, the state situation is simplified which matches this particular problem. To get the current state we simply choose a random country (ignoring the previous action and the possible changes of the enviroment) via `gen_rand_country()`. The reward is calculated by comparing the RL agent's prediction to the actual capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445a549",
   "metadata": {},
   "source": [
    "### Initiate and Train the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19651fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrls = CapitalsRLStep()\n",
    "qt = QTable(state_space=state_space, actions=actions)\n",
    "ql = QL(qtable=qt, rl_step=mrls)\n",
    "ql.train(3000000, get_correct_action_for_capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e739d5a",
   "metadata": {},
   "source": [
    "Before being able to train our RL agent, we need to define one more function that is used in the training loop and its main purpose is gather how the RL agent is doing during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
