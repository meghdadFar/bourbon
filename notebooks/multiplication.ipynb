{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceafd43b",
   "metadata": {},
   "source": [
    "## Training an RL Agent to Learn to Multiply\n",
    "\n",
    "This notebook presents a worked example of how to build an RL agent with Kandula. In this example we build an RL agent that learns what to multiply via reinforcement learning without using any math operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eda1b0",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First things first, let's import the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1bf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kandula import logging\n",
    "from kandula.steps import RLStep\n",
    "from kandula.qtable import QTable\n",
    "from kandula.q_learning import QL\n",
    "\n",
    "from random import randint\n",
    "from functools import reduce\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946e045",
   "metadata": {},
   "source": [
    "### Define the State Space\n",
    "\n",
    "The first thing that we should consider is how do we want to map our problem to a state-action space. Since a multiplication table presents a nice 2-dimensional space, it makes a good example for the reinforcement learning state space. We define the state space with a list, and each index of the list is a number that expresses how many possible values that dimension has. For instance, to represent a one-digit multiplication table in the range of 1-9, the state space can be defined as: `[9, 9]`. Let's however, make this space smaller (5x5) for the sake of less computation and faster convergance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca402a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c71fee",
   "metadata": {},
   "source": [
    "### Define the Actions\n",
    "\n",
    "The next thing is to define the actions. In this example, I consider guessing (the right) multiplication result as an action that my RL agent is suppose to learn. With respect to our state space above, the actions can then be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6045111",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [i for i in range(1,26)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3461d1",
   "metadata": {},
   "source": [
    "If for instance, my RL agent was supposed to take one of the two actions of e.g. shifting grear up or shifting gear down, my actions variable would have been `[shift_gear_up, shift_gear_down]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0964f",
   "metadata": {},
   "source": [
    "### Define the RL Step\n",
    "\n",
    "It's now time to define our RL step. An RL step should be a child of `kandula.steps.RLStep` class and implement its two abstract methods, namely `get_state()` and `get_reward()`. In addition to the definition of states and actions, this is where you make the RL agent really specific to your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c151fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRlStep(RLStep):\n",
    "    def get_state(self):\n",
    "        a, b = gen_rand_nums()\n",
    "        state = [a, b]\n",
    "        return state\n",
    "    def get_reward(self, state, action):\n",
    "        prod = reduce((lambda x,y: x*y), state)\n",
    "        reward = 1/(abs(prod-action)+1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114dc050",
   "metadata": {},
   "source": [
    "In the above code, the state situation is simplified which matches this particular problem. To get the current state we simply choose a two random numbers in the range of 1-5 (ignoring the previous action and the enviroment) via `gen_rand_nums()` defined below. The reward is calculated by comparing the RL agent's prediction to the actual capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19e16e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_nums():\n",
    "    num_1 = randint(1,5)\n",
    "    num_2 = randint(1,5)\n",
    "    return num_1, num_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926914ed",
   "metadata": {},
   "source": [
    "### Initiate and Train the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab86d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q_learning      - 136 - INFO - Training the RL agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 69999/69999 [00:09<00:00, 7688.67it/s, Error=12]\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "mrls = MyRlStep()\n",
    "qt = QTable(state_space=state_space, actions=actions)\n",
    "ql = QL(qtable=qt, rl_step=mrls)\n",
    "ql.train(num_epochs=70000,\n",
    "         get_correct_action=get_correct_action_for_multiply,\n",
    "        log_dir='rl_multiplication')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb36c08",
   "metadata": {},
   "source": [
    "Before being able to train our RL agent, we need to define one more function that is used in the training loop and its main purpose is to decide what is the best action to take in a given state. This function must be implemented with respect to the problem that you are solving. For instance, in this example, at each state the best action is simply the actual multiplication of the two indexes that represent the state. Hence, we can define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_action_for_multiply(state: list):\n",
    "    return state[0]*state[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba27b87",
   "metadata": {},
   "source": [
    "By setting `verbose=True` allow the train function to log the errors at each epoch, however, this is not necessary, as the error results are always stored in a tensorboard plot. Outside Notebooks, you can access the plots by simply running `tensorboard --logdir=rl_multiplication`. You can then access the tensorboard under http://localhost:6006/. To observe the plots in the Notebooks directly, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir rl_multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd86e7",
   "metadata": {},
   "source": [
    "The RL agent seems to be trained and the error has dropped well and relatively fast which is expected for this simple problem. Let's now write a script that uses the trained RL agent and answers multiplication queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af610f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    inp = input (\"Enter two numbers separated by a comma, to see their product: \")\n",
    "    if inp.lower() == \"stop\":\n",
    "        print(\"Have a nice day! Bye.\")\n",
    "        break\n",
    "    try:\n",
    "        a, b = inp.split(',')\n",
    "    except:\n",
    "        logging.error('Input format seems to be wrong, please try again.')\n",
    "        continue\n",
    "    a = int(a)\n",
    "    b = int(b)\n",
    "    if (a > 5) or (b > 5):\n",
    "        logging.error('The RL agent knows how to multiply only numbers <= 5.')\n",
    "        continue\n",
    "    state_index = ql.q_table.get_state_index([a,b])\n",
    "    action_index = torch.argmax(ql.q_table.q_table[state_index]).item()\n",
    "    res = ql.q_table.actions[action_index]\n",
    "    print(f'{a} x {b} = {res}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
